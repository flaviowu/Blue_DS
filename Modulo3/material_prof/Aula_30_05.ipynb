{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 30-05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRwGRAWwFPYX",
        "outputId": "b4c0b83d-f24b-40ef-d6a2-ffac5249a2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "\n",
        "import spacy\n",
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('pt')\n",
        "\n",
        "# criação de um documento spacy\n",
        "doc = nlp(\"Para enxergar claro, bastar mudar a direção do olhar.\")\n",
        "\n",
        "# visualização por meio de tokens\n",
        "for token in doc:\n",
        "    if token.is_alpha:\n",
        "      print(token.text + \": \" + \"texto\")\n",
        "    if token.is_punct:\n",
        "      print(token.text + \": \" + \"pontuação\")\n",
        "    if token.like_num:\n",
        "      print(token.text + \": \" + \"número\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYpM9xhCFX89",
        "outputId": "775edd40-d663-40a1-d33f-dc40ff276215"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para: texto\n",
            "enxergar: texto\n",
            "claro: texto\n",
            ",: pontuação\n",
            "bastar: texto\n",
            "mudar: texto\n",
            "a: texto\n",
            "direção: texto\n",
            "do: texto\n",
            "olhar: texto\n",
            ".: pontuação\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# considerando somente parte dos tokens\n",
        "span = doc[0:3]\n",
        "print(span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqvpVKxDFjFd",
        "outputId": "b9009ac9-800a-4a0f-8f1e-cea64c0a1654"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Para enxergar claro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# identificando termos gramaticais\n",
        "doc1 = nlp('Eu comi uma deliciosa pizza')\n",
        "\n",
        "for token in doc1:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgdeU2bRHhqW",
        "outputId": "0db609c5-2acd-451f-c9c1-9a0fae9f36e1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu PRON\n",
            "comi VERB\n",
            "uma DET\n",
            "deliciosa ADJ\n",
            "pizza NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# extração de entidades\n",
        "doc2 = nlp('Eu tenho usado o serviço de armazenamento na nuvem da Google, é a opção mais barata no Brasil, pago somente R$ 9.99 por mês.')\n",
        "\n",
        "for ent in doc2.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "displacy.render(doc2, style = \"ent\", jupyter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "VHMJu6o1HmaP",
        "outputId": "c46edf66-7d6b-4c42-b242-b9deb588d0a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "Brasil LOC\n",
            "R$ PER\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Eu tenho usado o serviço de armazenamento na nuvem da \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", é a opção mais barata no \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Brasil\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", pago somente \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    R$\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " 9.99 por mês.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "# criando algumas entidades manualmente\n",
        "doc3 = nlp('Olá forasteiro, está manhã teremos café, pão, ovos mexidos e manteiga, deseja que anotemos o pedido?')\n",
        "\n",
        "span_with_label = Span(doc3, 0, 2, label=\"GREETING\")\n",
        "doc3.ents = [span_with_label]\n",
        "\n",
        "for ent in doc3.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDC0b_pcHzP6",
        "outputId": "f3fe3bbd-4e2f-4411-ba21-898c74838314"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Olá forasteiro GREETING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# obtendo o radical das palavras \n",
        "doc4 = nlp('Você precisa entender, a maioria destas pessoas não está preparada para despertar. E muitas delas estão tão inertes, tão desesperadamente dependentes do sistema, que irão lutar para protegê-lo.')\n",
        "\n",
        "for token in doc4:\n",
        "    if token.pos_ == 'VERB':\n",
        "        print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOizFNPyIotv",
        "outputId": "8fff7849-f1bd-4911-b463-6764238a5ad0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precisa preciso\n",
            "entender entender\n",
            "está estar\n",
            "preparada preparar\n",
            "despertar despertar\n",
            "estão estar\n",
            "lutar lutar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similaridade entre textos\n",
        "doc5 = nlp('Eu gosto de pizza.')\n",
        "doc6 = nlp('Eu gosto de fast-food.')\n",
        "doc7 = nlp('Eu prefiro comer salada.')\n",
        "\n",
        "print(doc5.similarity(doc6))\n",
        "print(doc5.similarity(doc7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EUR29FDIxVo",
        "outputId": "fc774c9d-4b0a-427d-cba5-ca753833099d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9145424814876244\n",
            "0.5861113196570503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similaridade entre partes\n",
        "doc8 = nlp(\"Temos sorvete, pizza, batata-frita e refrigerante.\")\n",
        "token1 = doc8[3]\n",
        "token2 = doc8[7]\n",
        "\n",
        "print(token1)\n",
        "print(token2)\n",
        "print(token1.similarity(token2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLOxsMYXI0XC",
        "outputId": "c34b09f7-3c04-4d03-db56-b1dea89c06f8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pizza\n",
            "refrigerante\n",
            "0.31935522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similaridade e não sentimento\n",
        "doc9  = nlp('Eu gosto cachorros.')\n",
        "doc10 = nlp('Eu odeio cachorros.')\n",
        "\n",
        "print(doc9.similarity(doc10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhNkjCW1I4ef",
        "outputId": "0048b9d3-e37c-4f9c-b25c-cdc63644f1f3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9342498505491282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# stopwords\n",
        "print(len(stopwords))\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy7PdgZUNlma",
        "outputId": "bac14674-7b0b-4449-865d-0ed19ad0df2c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "204\n",
            "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um', 'para', 'com', 'não', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como', 'mas', 'ao', 'ele', 'das', 'à', 'seu', 'sua', 'ou', 'quando', 'muito', 'nos', 'já', 'eu', 'também', 'só', 'pelo', 'pela', 'até', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo', 'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'você', 'essa', 'num', 'nem', 'suas', 'meu', 'às', 'minha', 'numa', 'pelos', 'elas', 'qual', 'nós', 'lhe', 'deles', 'essas', 'esses', 'pelas', 'este', 'dele', 'tu', 'te', 'vocês', 'vos', 'lhes', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos', 'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'está', 'estamos', 'estão', 'estive', 'esteve', 'estivemos', 'estiveram', 'estava', 'estávamos', 'estavam', 'estivera', 'estivéramos', 'esteja', 'estejamos', 'estejam', 'estivesse', 'estivéssemos', 'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'há', 'havemos', 'hão', 'houve', 'houvemos', 'houveram', 'houvera', 'houvéramos', 'haja', 'hajamos', 'hajam', 'houvesse', 'houvéssemos', 'houvessem', 'houver', 'houvermos', 'houverem', 'houverei', 'houverá', 'houveremos', 'houverão', 'houveria', 'houveríamos', 'houveriam', 'sou', 'somos', 'são', 'era', 'éramos', 'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'fôramos', 'seja', 'sejamos', 'sejam', 'fosse', 'fôssemos', 'fossem', 'for', 'formos', 'forem', 'serei', 'será', 'seremos', 'serão', 'seria', 'seríamos', 'seriam', 'tenho', 'tem', 'temos', 'tém', 'tinha', 'tínhamos', 'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera', 'tivéramos', 'tenha', 'tenhamos', 'tenham', 'tivesse', 'tivéssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei', 'terá', 'teremos', 'terão', 'teria', 'teríamos', 'teriam']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pontuação\n",
        "import string\n",
        "print(len(string.punctuation))\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jXhZo8N5pF",
        "outputId": "3d0b0404-5e25-4026-a571-a80028387afe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# frase original\n",
        "texto = \"Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco.\"\n",
        "print(texto)\n",
        "\t\n",
        "# removendo alguns caracteres especiais indesejados\n",
        "texto = texto.replace(\",\",\" \").replace(\";\",\",\")\n",
        "print(texto)\n",
        "\n",
        "# transformando o texto em letras minúsculas\n",
        "texto = texto.lower()\n",
        "print(texto)\n",
        "\n",
        "# removendo stopwords\n",
        "texto = ' '.join([word for word in texto.split() if word not in stopwords])\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w22ePaUdOno9",
        "outputId": "5b3a392e-c47f-4cd9-fa4b-2902e282ad33"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim, isto é, se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento, duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto, mas um defunto autor, para quem a campa foi outro berço; a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés, que também contou a sua morte, não a pôs no intróito, mas no cabo: diferença radical entre este livro e o Pentateuco.\n",
            "Algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim  isto é  se poria em primeiro lugar o meu nascimento ou a minha morte. Suposto o uso vulgar seja começar pelo nascimento  duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto  mas um defunto autor  para quem a campa foi outro berço, a segunda é que o escrito ficaria assim mais galante e mais novo. Moisés  que também contou a sua morte  não a pôs no intróito  mas no cabo: diferença radical entre este livro e o Pentateuco.\n",
            "algum tempo hesitei se devia abrir estas memórias pelo princípio ou pelo fim  isto é  se poria em primeiro lugar o meu nascimento ou a minha morte. suposto o uso vulgar seja começar pelo nascimento  duas considerações me levaram a adotar diferente método: a primeira é que eu não sou propriamente um autor defunto  mas um defunto autor  para quem a campa foi outro berço, a segunda é que o escrito ficaria assim mais galante e mais novo. moisés  que também contou a sua morte  não a pôs no intróito  mas no cabo: diferença radical entre este livro e o pentateuco.\n",
            "algum tempo hesitei devia abrir memórias princípio fim poria primeiro lugar nascimento morte. suposto uso vulgar começar nascimento duas considerações levaram adotar diferente método: primeira propriamente autor defunto defunto autor campa outro berço, segunda escrito ficaria assim galante novo. moisés contou morte pôs intróito cabo: diferença radical livro pentateuco.\n"
          ]
        }
      ]
    }
  ]
}